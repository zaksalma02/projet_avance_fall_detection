# -*- coding: utf-8 -*-
"""Untitled2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/18WLUujmAzpPzasxfU3l0MwQvIVDw8HUp
"""

import tensorflow as tf
from tensorflow.keras import layers, models, optimizers
import pandas as pd
import numpy as np
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
from imblearn.over_sampling import SMOTE
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay
import matplotlib.pyplot as plt

# Chargement des fichiers de données
fall_data = pd.read_csv('fall.txt', header=None)
normal_data = pd.read_csv('normal.txt', header=None)

# Ajout des labels
fall_data['label'] = 'fall'
normal_data['label'] = 'normal'

# Combinaison des données
data = pd.concat([fall_data, normal_data], axis=0, ignore_index=True)

# Fonction pour diviser les données en échantillons de taille 120
def split_data_by_length(data, seq_len=120):
    num_samples = len(data) // seq_len  # Calculer le nombre d'échantillons possibles
    samples = []

    for i in range(num_samples):
        start_idx = i * seq_len
        end_idx = start_idx + seq_len
        sample = data.iloc[start_idx:end_idx]

        # Ajouter l'échantillon à la liste
        if len(sample) == seq_len:
            samples.append(sample)

    return samples

# Diviser les données en échantillons de taille 120
samples = split_data_by_length(data, seq_len=120)

# Vérification du nombre d'échantillons
print(f"Nombre d'échantillons : {len(samples)}")
if len(samples) < 2:
    raise ValueError("Il n'y a pas assez d'échantillons pour procéder à l'entraînement et au test.")

# Préparation des données pour l'entraînement
def prepare_data(samples, seed=1218):
    # Convertir les échantillons en matrices numpy et les labels associés
    new_data = []
    labels = []

    for sample in samples:
        sample_data = sample.drop('label', axis=1).to_numpy()
        new_data.append(sample_data)
        labels.append(1 if sample['label'].iloc[0] == 'fall' else 0)  # 1 pour fall, 0 pour normal

    new_data = np.array(new_data)
    labels = np.array(labels)

    # Vérifier la répartition des labels
    unique_labels = np.unique(labels)
    print(f"Répartition des labels : {dict(zip(*np.unique(labels, return_counts=True)))}")

    # Si les classes sont équilibrées, ne pas utiliser SMOTE
    if len(unique_labels) > 1 and np.all(np.bincount(labels) > 0):
        # Rééquilibrage des données avec SMOTE
        sm = SMOTE(random_state=seed)
        X, y = sm.fit_resample(new_data.reshape(-1, new_data.shape[1] * new_data.shape[2]), labels)
    else:
        X, y = new_data.reshape(-1, new_data.shape[1] * new_data.shape[2]), labels

    # Division en ensemble d'entraînement et de test
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=seed, stratify=y)

    # Normalisation
    scaler = StandardScaler()
    X_train = scaler.fit_transform(X_train)
    X_test = scaler.transform(X_test)

    # Reformater pour Conv1D
    X_train = X_train.reshape(-1, new_data.shape[1], new_data.shape[2])
    X_test = X_test.reshape(-1, new_data.shape[1], new_data.shape[2])

    return X_train, X_test, y_train, y_test, X_train.shape[1:]

X_train, X_test, y_train, y_test, input_shape = prepare_data(samples)

# Vérification des dimensions
print("X_train shape:", X_train.shape)
print("X_test shape:", X_test.shape)

# Création du modèle CBAM-IAM-CNN-Dense
CBAM_EDU = models.Sequential([
    layers.Conv1D(4, kernel_size=3, activation='relu', input_shape=input_shape),  # Seulement 4 filtres
    layers.MaxPooling1D(pool_size=2),  # Réduction de la taille
    layers.Flatten(),  # Aplatissement des caractéristiques
    layers.Dense(4, activation='relu', kernel_regularizer=tf.keras.regularizers.L2(0.01)),  # Petite couche dense
    layers.Dense(1, activation='sigmoid')  # Sortie binaire
])

# Compilation du modèle
optimizer = optimizers.Adam(learning_rate=0.001)
CBAM_EDU.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])

# Entraînement du modèle
EPOCHS = 100
BATCH_SIZE = 64
history = CBAM_EDU.fit(
    X_train, y_train,
    epochs=EPOCHS,
    batch_size=BATCH_SIZE,
    validation_data=(X_test, y_test)
)

# Prédictions et matrice de confusion
y_pred = CBAM_EDU.predict(X_test)
y_pred_classes = (y_pred > 0.5).astype(int)  # Convertir les probabilités en classes binaires

# Générer la matrice de confusion
cm = confusion_matrix(y_test, y_pred_classes)
disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=["Normal", "Fall"])
disp.plot(cmap='RdPu')  # Use 'coolwarm' for a pink-ish colormap
plt.title("Confusion Matrix")
plt.show()

# Visualisation des courbes d'accuracy et de perte
train_accuracy = history.history['accuracy']
val_accuracy = history.history['val_accuracy']
train_loss = history.history['loss']
val_loss = history.history['val_loss']
epochs_range = range(1, EPOCHS + 1)

plt.figure(figsize=(12, 6))
plt.plot(epochs_range, train_accuracy, label='Training Accuracy', marker='o')
plt.plot(epochs_range, val_accuracy, label='Validation Accuracy', marker='x')
plt.xlabel('Epochs')
plt.ylabel('Accuracy')
plt.title('Model Accuracy')
plt.legend()
plt.grid(True)
plt.show()

plt.figure(figsize=(12, 6))
plt.plot(epochs_range, train_loss, label='Training Loss', marker='o')
plt.plot(epochs_range, val_loss, label='Validation Loss', marker='x')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.title('Model Loss')
plt.legend()
plt.grid(True)
plt.show()
import tensorflow as tf

# Sauvegarder le modèle en .h5
CBAM_EDU.save('fall_detection_model.h5')
# Charger le modèle .h5 (modèle entraîné)
model = tf.keras.models.load_model('fall_detection_model.h5')

# Convertir le modèle en format TensorFlow Lite
converter = tf.lite.TFLiteConverter.from_keras_model(model)
tflite_model = converter.convert()

# Sauvegarder le modèle au format .tflite
with open('fall_detection_model.tflite', 'wb') as f:
    f.write(tflite_model)

# Charger le modèle .tflite pour générer un fichier .h
with open('fall_detection_model.tflite', 'rb') as f:
    model_data = f.read()

# Créer un fichier .h contenant le modèle binaire
with open('model.h', 'w') as h_file:
    h_file.write('#include <stdint.h>\n\n')
    h_file.write('const uint8_t model[] = {')
    h_file.write(','.join([f'0x{byte:02x}' for byte in model_data]))
    h_file.write('};\n')

print("Le modèle a été converti et sauvegardé en tant que modèle .tflite et .h.")